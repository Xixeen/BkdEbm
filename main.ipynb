{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install torch-geometric\n",
    "!pip install torch_spline_conv\n",
    "!pip install torch_scatter \n",
    "!pip install torch_cluster #slow\n",
    "!pip install torch_sparse #slow\n",
    "!pip install matplotlib\n",
    "!pip install ogb\n",
    "!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5QApBmLhOGP"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from models import GCN\n",
    "\n",
    "import torch_geometric\n",
    "\n",
    "import torch_sparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "from utils import get_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_process import generate_data, load_data\n",
    "from train_func import test, train, Lhop_Block_matrix_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9n0WIOFZPqz2"
   },
   "outputs": [],
   "source": [
    "def get_K_hop_neighbors(adj_matrix, index, K):\n",
    "    adj_matrix = adj_matrix + torch.eye(adj_matrix.shape[0],adj_matrix.shape[1])  #make sure the diagonal part >= 1\n",
    "    hop_neightbor_index=index\n",
    "    for i in range(K):\n",
    "        hop_neightbor_index=torch.unique(torch.nonzero(adj[hop_neightbor_index])[:,1])\n",
    "    return hop_neightbor_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "def normalize(mx):  #adj matrix\n",
    "    \n",
    "    mx = mx + torch.eye(mx.shape[0],mx.shape[1])\n",
    "    \n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return torch.tensor(mx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5CMAPT6gafH"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "\n",
    "#for compare 2-10 layer performance in appendix\n",
    "#iterations = 400\n",
    "#Adam, lr = 0.01\n",
    "\n",
    "\n",
    "def centralized_GCN(features, adj, labels, idx_train, idx_val, idx_test, num_layers):\n",
    "    \n",
    "        model = GCN(nfeat=features.shape[1],\n",
    "                nhid=args_hidden,\n",
    "                nclass=labels.max().item() + 1,\n",
    "                dropout=args_dropout,\n",
    "                NumLayers=num_layers)\n",
    "        model.reset_parameters()\n",
    "        if args_cuda:\n",
    "                #from torch_geometric.nn import DataParallel\n",
    "                #model = DataParallel(model)\n",
    "                #model= torch.nn.DataParallel(model)\n",
    "                model=model.cuda()\n",
    "                \n",
    "                #features= torch.nn.DataParallel(features)\n",
    "                \n",
    "                features = features.cuda()\n",
    "                \n",
    "                #edge_index= torch.nn.DataParallel(edge_index)\n",
    "                \n",
    "                adj = adj.cuda()\n",
    "                labels = labels.cuda()\n",
    "                idx_train = idx_train.cuda()\n",
    "                idx_val = idx_val.cuda()\n",
    "                idx_test = idx_test.cuda()\n",
    "        #optimizer and train\n",
    "        \n",
    "        optimizer = optim.SGD(model.parameters(),\n",
    "                              lr=args_lr, weight_decay=args_weight_decay)\n",
    "        \n",
    "        \n",
    "        #optimizer = optim.Adam(model.parameters(),\n",
    "        #                      lr=args_lr, weight_decay=args_weight_decay)\n",
    "        # Train model\n",
    "        best_val=0\n",
    "        for t in range(args_iterations): #make to equivalent to federated\n",
    "            loss_train, acc_train=train(t, model, optimizer, features, adj, labels, idx_train)\n",
    "            # validation\n",
    "            loss_train, acc_train= test(model, features, adj, labels, idx_train) #train after backward\n",
    "            print(t,\"train\",loss_train,acc_train)\n",
    "            loss_val, acc_val= test(model, features, adj, labels, idx_val) #validation\n",
    "            print(t,\"val\",loss_val,acc_val)\n",
    "            \n",
    "            a = open(dataset_name+'_IID_'+'centralized_' + str(num_layers) + 'layer_GCN_iter_'+str(args_iterations),'a+')\n",
    "            a.write(str(t)+'\\t'+\"train\"+'\\t'+str(loss_train)+'\\t'+str(acc_train)+'\\n')\n",
    "            a.write(str(t)+'\\t'+\"val\"+'\\t'+str(loss_val)+'\\t'+str(acc_val)+'\\n')\n",
    "            a.close()\n",
    "            \n",
    "        #test  \n",
    "        loss_test, acc_test= test(model, features, adj, labels, idx_test)\n",
    "        print(t,'\\t',\"test\",'\\t',loss_test,'\\t',acc_test)\n",
    "        a = open(dataset_name+'_IID_'+'centralized_' + str(num_layers) + 'layer_GCN_iter_'+str(args_iterations),'a+')\n",
    "        a.write(str(t)+'\\t'+\"test\"+'\\t'+str(loss_test)+'\\t'+str(acc_test)+'\\n')\n",
    "        a.close()\n",
    "        \n",
    "        print(\"save file as\",dataset_name+'_IID_'+'centralized_' + str(num_layers) + 'layer_GCN_iter_'+str(args_iterations))\n",
    "        del model\n",
    "        del features \n",
    "        del adj\n",
    "        del labels\n",
    "        del idx_train\n",
    "        del idx_val\n",
    "        del idx_test\n",
    "        \n",
    "        return loss_test, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setdiff1d(t1, t2):\n",
    "    \n",
    "    combined = torch.cat((t1, t2))\n",
    "    uniques, counts = combined.unique(return_counts=True)\n",
    "    difference = uniques[counts == 1]\n",
    "    #intersection = uniques[counts > 1]\n",
    "    return difference\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect1d(t1, t2):\n",
    "    \n",
    "    combined = torch.cat((t1, t2))\n",
    "    uniques, counts = combined.unique(return_counts=True)\n",
    "    #difference = uniques[counts == 1]\n",
    "    intersection = uniques[counts > 1]\n",
    "    return intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BDS_federated_GCN(K, features, adj, labels, idx_train, idx_val, idx_test, iid_percent, sample_rate =0.5, L_hop=1, num_layers=2):\n",
    "        # K: number of models\n",
    "        #choose adj matrix\n",
    "        #multilayer_GCN:n*n\n",
    "        #define model\n",
    "        global_model = GCN(nfeat=features.shape[1],\n",
    "                nhid=args_hidden,\n",
    "                nclass=labels.max().item() + 1,\n",
    "                dropout=args_dropout,\n",
    "                NumLayers=num_layers)\n",
    "        global_model.reset_parameters()\n",
    "        models=[]\n",
    "        for i in range(K):\n",
    "            models.append(GCN(nfeat=features.shape[1],\n",
    "                nhid=args_hidden,\n",
    "                nclass=labels.max().item() + 1,\n",
    "                dropout=args_dropout,\n",
    "                NumLayers=num_layers))\n",
    "        if args_cuda:\n",
    "                for i in range(K):\n",
    "                    models[i]=models[i].cuda()\n",
    "                global_model=global_model.cuda()\n",
    "                features = features.cuda()\n",
    "                adj = adj.cuda()\n",
    "                labels = labels.cuda()\n",
    "                idx_train = idx_train.cuda()\n",
    "                idx_val = idx_val.cuda()\n",
    "                idx_test = idx_test.cuda()\n",
    "        #optimizer and train\n",
    "        optimizers=[]\n",
    "        for i in range(K):\n",
    "            optimizers.append(optim.SGD(models[i].parameters(),\n",
    "                              lr=args_lr, weight_decay=args_weight_decay))\n",
    "        # Train model\n",
    "        \n",
    "        row, col, edge_attr = adj.t().coo()\n",
    "        edge_index = torch.stack([row, col], dim=0)\n",
    "        \n",
    "        \n",
    "        split_data_indexes=[]\n",
    "        \n",
    "        nclass=labels.max().item() + 1\n",
    "        split_data_indexes = []\n",
    "        non_iid_percent = 1 - float(iid_percent)\n",
    "        iid_indexes = [] #random assign\n",
    "        shuffle_labels = [] #make train data points split into different devices\n",
    "        for i in range(K):\n",
    "            current = torch.nonzero(labels == i).reshape(-1)\n",
    "            current = current[np.random.permutation(len(current))] #shuffle\n",
    "            shuffle_labels.append(current)\n",
    "                \n",
    "        average_device_of_class = K // nclass\n",
    "        if K % nclass != 0: #for non-iid\n",
    "            average_device_of_class += 1\n",
    "        for i in range(K):  \n",
    "            label_i= i // average_device_of_class    \n",
    "            labels_class = shuffle_labels[label_i]\n",
    "\n",
    "            average_num= int(len(labels_class)//average_device_of_class * non_iid_percent)\n",
    "            split_data_indexes.append((labels_class[average_num * (i % average_device_of_class):average_num * (i % average_device_of_class + 1)]))\n",
    "        \n",
    "        if args_cuda:\n",
    "            iid_indexes = setdiff1d(torch.tensor(range(len(labels))).cuda(), torch.cat(split_data_indexes))\n",
    "        else:\n",
    "            iid_indexes = setdiff1d(torch.tensor(range(len(labels))), torch.cat(split_data_indexes))\n",
    "        \n",
    "        iid_indexes = iid_indexes[np.random.permutation(len(iid_indexes))]\n",
    "        \n",
    "        for i in range(K):  #for iid\n",
    "            label_i= i // average_device_of_class\n",
    "            labels_class = shuffle_labels[label_i]\n",
    "\n",
    "            average_num= int(len(labels_class)//average_device_of_class * (1 - non_iid_percent))\n",
    "            split_data_indexes[i] = list(split_data_indexes[i]) + list(iid_indexes[:average_num])\n",
    "                    \n",
    "            iid_indexes = iid_indexes[average_num:]\n",
    "            \n",
    "        communicate_indexes = []\n",
    "        in_com_train_data_indexes = []\n",
    "\n",
    "        for i in range(K):\n",
    "            if args_cuda:\n",
    "                split_data_indexes[i] = torch.tensor(split_data_indexes[i]).cuda()\n",
    "            else:\n",
    "                split_data_indexes[i] = torch.tensor(split_data_indexes[i])\n",
    "                \n",
    "            split_data_indexes[i] = split_data_indexes[i].sort()[0]\n",
    "            \n",
    "            #communicate_index=get_K_hop_neighbors(adj, split_data_indexes[i], L_hop) #normalized adj\n",
    "            \n",
    "            communicate_index = torch_geometric.utils.k_hop_subgraph(split_data_indexes[i],L_hop,edge_index)[0]\n",
    "            communicate_indexes.append(communicate_index)\n",
    "            communicate_indexes[i] = communicate_indexes[i].sort()[0]\n",
    "            \n",
    "            inter = intersect1d(split_data_indexes[i], idx_train)  ###only count the train data of nodes in current server(not communicate nodes)   \n",
    "            in_com_train_data_indexes.append(torch.searchsorted(communicate_indexes[i], inter).clone())   #local id in block matrix\n",
    "\n",
    "            \n",
    "            \n",
    "        #assign global model weights to local models at initial step\n",
    "        for i in range(K):\n",
    "            models[i].load_state_dict(global_model.state_dict())\n",
    "        \n",
    "        for t in range(args_iterations):\n",
    "            acc_trains=[]\n",
    "            for i in range(K):\n",
    "                for epoch in range(args_epochs):\n",
    "                    diff = setdiff1d(split_data_indexes[i], communicate_indexes[i])\n",
    "                    sample_index = torch.cat((split_data_indexes[i], diff[torch.randperm(len(diff))[:int(len(diff) * sample_rate)]])).clone()\n",
    "\n",
    "                    sample_index = sample_index.sort()[0]\n",
    "                    \n",
    "                    inter = intersect1d(split_data_indexes[i], idx_train)  ###only count the train data of nodes in current server(not communicate nodes)\n",
    "                    in_sample_train_data_index = torch.searchsorted(sample_index, inter).clone()   #local id in block matrix\n",
    "\n",
    "                    if len(in_sample_train_data_index) == 0:\n",
    "                        continue\n",
    "                    try:\n",
    "                        adj[sample_index][:,sample_index]\n",
    "                    except: #adj is empty\n",
    "                        continue\n",
    "                    \n",
    "                    acc_train = FedSage_train(epoch, models[i], optimizers[i], \n",
    "                                                        features, adj, labels, sample_index, in_sample_train_data_index)\n",
    "                acc_trains.append(acc_train)\n",
    "                \n",
    "            states=[]\n",
    "            gloabl_state=dict()\n",
    "            for i in range(K):\n",
    "                states.append(models[i].state_dict())\n",
    "            # Average all parameters\n",
    "            for key in global_model.state_dict():\n",
    "                gloabl_state[key] = in_com_train_data_indexes[0].shape[0] * states[0][key]\n",
    "                count_D=in_com_train_data_indexes[0].shape[0]\n",
    "                for i in range(1,K):\n",
    "                    gloabl_state[key] += in_com_train_data_indexes[i].shape[0] * states[i][key]\n",
    "                    count_D += in_com_train_data_indexes[i].shape[0]\n",
    "                gloabl_state[key] /= count_D\n",
    "\n",
    "            global_model.load_state_dict(gloabl_state)\n",
    "            \n",
    "            # Testing\n",
    "            \n",
    "            loss_train, acc_train = test(global_model, features, adj, labels, idx_train)\n",
    "            print(t,'\\t',\"train\",'\\t',loss_train,'\\t',acc_train)\n",
    "            \n",
    "            loss_val, acc_val = test(global_model, features, adj, labels, idx_val) #validation\n",
    "            print(t,'\\t',\"val\",'\\t',loss_val,'\\t',acc_val)\n",
    "            \n",
    "\n",
    "            a = open(dataset_name+'_IID_'+str(iid_percent)+'_' + str(L_hop) +'hop_BDS_federated_' + str(num_layers) + 'layer_GCN_iter_'+str(args_iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K),'a+')\n",
    "\n",
    "            \n",
    "            a.write(str(t)+'\\t'+\"train\"+'\\t'+str(loss_train)+'\\t'+str(acc_train)+'\\n')\n",
    "            a.write(str(t)+'\\t'+\"val\"+'\\t'+str(loss_val)+'\\t'+str(acc_val)+'\\n')\n",
    "            a.close()\n",
    "            for i in range(K):\n",
    "                models[i].load_state_dict(gloabl_state)\n",
    "        #test  \n",
    "        loss_test, acc_test= test(global_model, features, adj, labels, idx_test)\n",
    "        print(t,'\\t',\"test\",'\\t',loss_test,'\\t',acc_test)\n",
    "        a = open(dataset_name+'_IID_'+str(iid_percent)+'_' + str(L_hop) +'hop_BDS_federated_' + str(num_layers) + 'layer_GCN_iter_'+str(args_iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K),'a+')\n",
    "        a.write(str(t)+'\\t'+\"test\"+'\\t'+str(loss_test)+'\\t'+str(acc_test)+'\\n')\n",
    "        a.close()\n",
    "        print(\"save file as\",dataset_name+'_IID_'+str(iid_percent)+'_' + str(L_hop) +'hop_BDS_federated_' + str(num_layers) + 'layer_GCN_iter_'+str(args_iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K))\n",
    "        \n",
    "        del global_model\n",
    "        del features \n",
    "        del adj\n",
    "        del labels\n",
    "        del idx_train\n",
    "        del idx_val\n",
    "        del idx_test\n",
    "        while(len(models)>=1):\n",
    "            del models[0]\n",
    "        \n",
    "        return loss_test, acc_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FedSage_plus(K, features, adj, labels, idx_train, idx_val, idx_test, iid_percent, L_hop = 1, num_layers=2):\n",
    "        # K: number of models\n",
    "        #choose adj matrix\n",
    "        #multilayer_GCN:n*n\n",
    "        #define model\n",
    "        global_model = GCN(nfeat=features.shape[1],\n",
    "                nhid=args_hidden,\n",
    "                nclass=labels.max().item() + 1,\n",
    "                dropout=args_dropout,\n",
    "                NumLayers=num_layers)\n",
    "        global_model.reset_parameters()\n",
    "        models=[]\n",
    "        for i in range(K):\n",
    "            models.append(GCN(nfeat=features.shape[1],\n",
    "                nhid=args_hidden,\n",
    "                nclass=labels.max().item() + 1,\n",
    "                dropout=args_dropout,\n",
    "                NumLayers=num_layers))\n",
    "        if args_cuda:\n",
    "                for i in range(K):\n",
    "                    models[i]=models[i].to(device)\n",
    "                global_model=global_model.to(device)\n",
    "                features = features.to(device)\n",
    "                adj = adj.to(device)\n",
    "                labels = labels.to(device)\n",
    "                idx_train = idx_train.to(device)\n",
    "                idx_val = idx_val.to(device)\n",
    "                idx_test = idx_test.to(device)\n",
    "        #optimizer and train\n",
    "        optimizers=[]\n",
    "        for i in range(K):\n",
    "            optimizers.append(optim.SGD(models[i].parameters(),\n",
    "                              lr=args_lr, weight_decay=args_weight_decay))\n",
    "        # Train model\n",
    "        \n",
    "        row, col, edge_attr = adj.t().coo()\n",
    "        edge_index = torch.stack([row, col], dim=0)\n",
    "        \n",
    "        \n",
    "        split_data_indexes=[]\n",
    "        \n",
    "        nclass=labels.max().item() + 1\n",
    "        split_data_indexes = []\n",
    "        non_iid_percent = 1 - float(iid_percent)\n",
    "        iid_indexes = [] #random assign\n",
    "        shuffle_labels = [] #make train data points split into different devices\n",
    "        for i in range(K):\n",
    "            current = torch.nonzero(labels == i).reshape(-1)\n",
    "            current = current[np.random.permutation(len(current))] #shuffle\n",
    "            shuffle_labels.append(current)\n",
    "                \n",
    "        average_device_of_class = K // nclass\n",
    "        if K % nclass != 0: #for non-iid\n",
    "            average_device_of_class += 1\n",
    "        for i in range(K):  \n",
    "            label_i= i // average_device_of_class    \n",
    "            labels_class = shuffle_labels[label_i]\n",
    "\n",
    "            average_num= int(len(labels_class)//average_device_of_class * non_iid_percent)\n",
    "            split_data_indexes.append((labels_class[average_num * (i % average_device_of_class):average_num * (i % average_device_of_class + 1)]))\n",
    "        \n",
    "        if args_cuda:\n",
    "            iid_indexes = setdiff1d(torch.tensor(range(len(labels))).to(device), torch.cat(split_data_indexes))\n",
    "        else:\n",
    "            iid_indexes = setdiff1d(torch.tensor(range(len(labels))), torch.cat(split_data_indexes))\n",
    "        \n",
    "        iid_indexes = iid_indexes[np.random.permutation(len(iid_indexes))]\n",
    "        \n",
    "        for i in range(K):  #for iid\n",
    "            label_i= i // average_device_of_class\n",
    "            labels_class = shuffle_labels[label_i]\n",
    "\n",
    "            average_num= int(len(labels_class)//average_device_of_class * (1 - non_iid_percent))\n",
    "            split_data_indexes[i] = list(split_data_indexes[i]) + list(iid_indexes[:average_num])\n",
    "                    \n",
    "            iid_indexes = iid_indexes[average_num:]\n",
    "            \n",
    "        communicate_indexes = []\n",
    "        in_com_train_data_indexes = []\n",
    "        for i in range(K):\n",
    "            if args_cuda:\n",
    "                split_data_indexes[i] = torch.tensor(split_data_indexes[i]).to(device)\n",
    "            else:\n",
    "                split_data_indexes[i] = torch.tensor(split_data_indexes[i])\n",
    "                \n",
    "            split_data_indexes[i] = split_data_indexes[i].sort()[0]\n",
    "            \n",
    "            #communicate_index=get_K_hop_neighbors(adj, split_data_indexes[i], L_hop) #normalized adj\n",
    "            \n",
    "            communicate_index = torch_geometric.utils.k_hop_subgraph(split_data_indexes[i],L_hop,edge_index)[0]\n",
    "                \n",
    "            communicate_indexes.append(communicate_index)\n",
    "            communicate_indexes[i] = communicate_indexes[i].sort()[0]\n",
    "            \n",
    "            inter = intersect1d(split_data_indexes[i], idx_train)  ###only count the train data of nodes in current server(not communicate nodes)\n",
    "\n",
    "                \n",
    "            in_com_train_data_indexes.append(torch.searchsorted(communicate_indexes[i], inter).clone())   #local id in block matrix\n",
    "        \n",
    "        features_in_clients = []\n",
    "        #assume the linear generator learnt the optimal (the average of features of neighbor nodes)\n",
    "        #gaussian noise\n",
    "        for i in range(K):\n",
    "            #orignial features of outside neighbors of nodes in client i\n",
    "            original_feature_i = features[setdiff1d(split_data_indexes[i], communicate_indexes[i])].clone()\n",
    "            \n",
    "            gaussian_feature_i = original_feature_i + torch.normal(0, 0.1, original_feature_i.shape).to(device)\n",
    "            \n",
    "            copy_feature = features.clone()\n",
    "            \n",
    "            copy_feature[setdiff1d(split_data_indexes[i], communicate_indexes[i])] = gaussian_feature_i\n",
    "            \n",
    "            features_in_clients.append(copy_feature[communicate_indexes[i]])\n",
    "            print(features_in_clients[i].shape, communicate_indexes[i].shape)\n",
    "\n",
    "        #assign global model weights to local models at initial step\n",
    "        for i in range(K):\n",
    "            models[i].load_state_dict(global_model.state_dict())\n",
    "        \n",
    "        for t in range(args_iterations):\n",
    "            acc_trains=[]\n",
    "            for i in range(K):\n",
    "                for epoch in range(args_epochs):\n",
    "                    if len(in_com_train_data_indexes[i]) == 0:\n",
    "                        continue\n",
    "                    try:\n",
    "                        adj[communicate_indexes[i]][:,communicate_indexes[i]]\n",
    "                    except: #adj is empty\n",
    "                        continue\n",
    "                    acc_train = FedSage_train(epoch, models[i], optimizers[i], \n",
    "                                                        features_in_clients[i], adj, labels, communicate_indexes[i], in_com_train_data_indexes[i])\n",
    "                    \n",
    "                acc_trains.append(acc_train)\n",
    "            states=[]\n",
    "            gloabl_state=dict()\n",
    "            for i in range(K):\n",
    "                states.append(models[i].state_dict())\n",
    "            # Average all parameters\n",
    "            for key in global_model.state_dict():\n",
    "                gloabl_state[key] = in_com_train_data_indexes[0].shape[0] * states[0][key]\n",
    "                count_D=in_com_train_data_indexes[0].shape[0]\n",
    "                for i in range(1,K):\n",
    "                    gloabl_state[key] += in_com_train_data_indexes[i].shape[0] * states[i][key]\n",
    "                    count_D += in_com_train_data_indexes[i].shape[0]\n",
    "                gloabl_state[key] /= count_D\n",
    "\n",
    "            global_model.load_state_dict(gloabl_state)\n",
    "            \n",
    "            # Testing\n",
    "            \n",
    "            loss_train, acc_train = test(global_model, features, adj, labels, idx_train)\n",
    "            print(t,'\\t',\"train\",'\\t',loss_train,'\\t',acc_train)\n",
    "            \n",
    "            loss_val, acc_val = test(global_model, features, adj, labels, idx_val) #validation\n",
    "            print(t,'\\t',\"val\",'\\t',loss_val,'\\t',acc_val)\n",
    "            \n",
    "\n",
    "            a = open(dataset_name+'_IID_'+str(iid_percent)+'_' + str(L_hop) +'hop_FedSage_' + str(num_layers) + 'layer_GCN_iter_'+str(args_iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K),'a+')\n",
    "\n",
    "            \n",
    "            a.write(str(t)+'\\t'+\"train\"+'\\t'+str(loss_train)+'\\t'+str(acc_train)+'\\n')\n",
    "            a.write(str(t)+'\\t'+\"val\"+'\\t'+str(loss_val)+'\\t'+str(acc_val)+'\\n')\n",
    "            a.close()\n",
    "            for i in range(K):\n",
    "                models[i].load_state_dict(gloabl_state)\n",
    "        #test  \n",
    "        loss_test, acc_test= test(global_model, features, adj, labels, idx_test)\n",
    "        print(t,'\\t',\"test\",'\\t',loss_test,'\\t',acc_test)\n",
    "        a = open(dataset_name+'_IID_'+str(iid_percent)+'_' + str(L_hop) +'hop_FedSage_' + str(num_layers) + 'layer_GCN_iter_'+str(args_iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K),'a+')\n",
    "        a.write(str(t)+'\\t'+\"test\"+'\\t'+str(loss_test)+'\\t'+str(acc_test)+'\\n')\n",
    "        a.close()\n",
    "        print(\"save file as\",dataset_name+'_IID_'+str(iid_percent)+'_' + str(L_hop) +'hop_FedSage_' + str(num_layers) + 'layer_GCN_iter_'+str(args_iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K))\n",
    "        \n",
    "        del global_model\n",
    "        del features \n",
    "        del adj\n",
    "        del labels\n",
    "        del idx_train\n",
    "        del idx_val\n",
    "        del idx_test\n",
    "        while(len(models)>=1):\n",
    "            del models[0]\n",
    "        \n",
    "        return loss_test, acc_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lhop_Block_federated_GCN(K, features, adj, labels, idx_train, idx_val, idx_test, iid_percent, L_hop, num_layers):\n",
    "        # K: number of models\n",
    "        #choose adj matrix\n",
    "        #multilayer_GCN:n*n\n",
    "        #define model\n",
    "        global_model = GCN(nfeat=features.shape[1],\n",
    "                nhid=args_hidden,\n",
    "                nclass=labels.max().item() + 1,\n",
    "                dropout=args_dropout,\n",
    "                NumLayers=num_layers)\n",
    "        global_model.reset_parameters()\n",
    "        models=[]\n",
    "        for i in range(K):\n",
    "            models.append(GCN(nfeat=features.shape[1],\n",
    "                nhid=args_hidden,\n",
    "                nclass=labels.max().item() + 1,\n",
    "                dropout=args_dropout,\n",
    "                NumLayers=num_layers))\n",
    "        if args_cuda:\n",
    "                for i in range(K):\n",
    "                    models[i]=models[i].cuda()\n",
    "                global_model=global_model.cuda()\n",
    "                features = features.cuda()\n",
    "                adj = adj.cuda()\n",
    "                labels = labels.cuda()\n",
    "                idx_train = idx_train.cuda()\n",
    "                idx_val = idx_val.cuda()\n",
    "                idx_test = idx_test.cuda()\n",
    "        #optimizer and train\n",
    "        optimizers=[]\n",
    "        for i in range(K):\n",
    "            optimizers.append(optim.SGD(models[i].parameters(),\n",
    "                              lr=args_lr, weight_decay=args_weight_decay))\n",
    "        # Train model\n",
    "        \n",
    "        row, col, edge_attr = adj.t().coo()\n",
    "        edge_index = torch.stack([row, col], dim=0)\n",
    "        \n",
    "        \n",
    "        split_data_indexes=[]\n",
    "        \n",
    "        nclass=labels.max().item() + 1\n",
    "        split_data_indexes = []\n",
    "        non_iid_percent = 1 - float(iid_percent)\n",
    "        iid_indexes = [] #random assign\n",
    "        shuffle_labels = [] #make train data points split into different devices\n",
    "        for i in range(K):\n",
    "            current = torch.nonzero(labels == i).reshape(-1)\n",
    "            current = current[np.random.permutation(len(current))] #shuffle\n",
    "            shuffle_labels.append(current)\n",
    "                \n",
    "        average_device_of_class = K // nclass\n",
    "        if K % nclass != 0: #for non-iid\n",
    "            average_device_of_class += 1\n",
    "        for i in range(K):  \n",
    "            label_i= i // average_device_of_class    \n",
    "            labels_class = shuffle_labels[label_i]\n",
    "\n",
    "            average_num= int(len(labels_class)//average_device_of_class * non_iid_percent)\n",
    "            split_data_indexes.append((labels_class[average_num * (i % average_device_of_class):average_num * (i % average_device_of_class + 1)]))\n",
    "        \n",
    "        if args_cuda:\n",
    "            iid_indexes = setdiff1d(torch.tensor(range(len(labels))).cuda(), torch.cat(split_data_indexes))\n",
    "        else:\n",
    "            iid_indexes = setdiff1d(torch.tensor(range(len(labels))), torch.cat(split_data_indexes))\n",
    "        iid_indexes = iid_indexes[np.random.permutation(len(iid_indexes))]\n",
    "        \n",
    "        for i in range(K):  #for iid\n",
    "            label_i= i // average_device_of_class\n",
    "            labels_class = shuffle_labels[label_i]\n",
    "\n",
    "            average_num= int(len(labels_class)//average_device_of_class * (1 - non_iid_percent))\n",
    "            split_data_indexes[i] = list(split_data_indexes[i]) + list(iid_indexes[:average_num])\n",
    "                    \n",
    "            iid_indexes = iid_indexes[average_num:]\n",
    "            \n",
    "        communicate_indexes = []\n",
    "        in_com_train_data_indexes = []\n",
    "        for i in range(K):\n",
    "            if args_cuda:\n",
    "                split_data_indexes[i] = torch.tensor(split_data_indexes[i]).cuda()\n",
    "            else:\n",
    "                split_data_indexes[i] = torch.tensor(split_data_indexes[i])\n",
    "                \n",
    "            split_data_indexes[i] = split_data_indexes[i].sort()[0]\n",
    "            \n",
    "            #communicate_index=get_K_hop_neighbors(adj, split_data_indexes[i], L_hop) #normalized adj\n",
    "            \n",
    "            communicate_index = torch_geometric.utils.k_hop_subgraph(split_data_indexes[i],L_hop,edge_index)[0]\n",
    "                \n",
    "            communicate_indexes.append(communicate_index)\n",
    "            communicate_indexes[i] = communicate_indexes[i].sort()[0]\n",
    "            \n",
    "            inter = intersect1d(split_data_indexes[i], idx_train)  ###only count the train data of nodes in current server(not communicate nodes)\n",
    "\n",
    "                \n",
    "            in_com_train_data_indexes.append(torch.searchsorted(communicate_indexes[i], inter).clone())   #local id in block matrix\n",
    "\n",
    "        #assign global model weights to local models at initial step\n",
    "        for i in range(K):\n",
    "            models[i].load_state_dict(global_model.state_dict())\n",
    "        \n",
    "        for t in range(args_iterations):\n",
    "            acc_trains=[]\n",
    "            for i in range(K):\n",
    "                for epoch in range(args_epochs):\n",
    "                    if len(in_com_train_data_indexes[i]) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        adj[communicate_indexes[i]][:,communicate_indexes[i]]\n",
    "                    except: #adj is empty\n",
    "                        continue\n",
    "                    acc_train = Lhop_Block_matrix_train(epoch, models[i], optimizers[i], \n",
    "                                                        features, adj, labels, communicate_indexes[i], in_com_train_data_indexes[i])\n",
    "                    \n",
    "                acc_trains.append(acc_train)\n",
    "            states=[]\n",
    "            gloabl_state=dict()\n",
    "            for i in range(K):\n",
    "                states.append(models[i].state_dict())\n",
    "            # Average all parameters\n",
    "            for key in global_model.state_dict():\n",
    "                gloabl_state[key] = in_com_train_data_indexes[0].shape[0] * states[0][key]\n",
    "                count_D=in_com_train_data_indexes[0].shape[0]\n",
    "                for i in range(1,K):\n",
    "                    gloabl_state[key] += in_com_train_data_indexes[i].shape[0] * states[i][key]\n",
    "                    count_D += in_com_train_data_indexes[i].shape[0]\n",
    "                gloabl_state[key] /= count_D\n",
    "\n",
    "            global_model.load_state_dict(gloabl_state)\n",
    "            \n",
    "            # Testing\n",
    "            \n",
    "            loss_train, acc_train = test(global_model, features, adj, labels, idx_train)\n",
    "            print(t,'\\t',\"train\",'\\t',loss_train,'\\t',acc_train)\n",
    "            \n",
    "            loss_val, acc_val = test(global_model, features, adj, labels, idx_val) #validation\n",
    "            print(t,'\\t',\"val\",'\\t',loss_val,'\\t',acc_val)\n",
    "            \n",
    "\n",
    "            a = open(dataset_name+'_IID_'+str(iid_percent)+'_' + str(L_hop) +'hop_Block_federated_' + str(num_layers) + 'layer_GCN_iter_'+str(args_iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K),'a+')\n",
    "\n",
    "            \n",
    "            a.write(str(t)+'\\t'+\"train\"+'\\t'+str(loss_train)+'\\t'+str(acc_train)+'\\n')\n",
    "            a.write(str(t)+'\\t'+\"val\"+'\\t'+str(loss_val)+'\\t'+str(acc_val)+'\\n')\n",
    "            a.close()\n",
    "            for i in range(K):\n",
    "                models[i].load_state_dict(gloabl_state)\n",
    "        #test  \n",
    "        loss_test, acc_test= test(global_model, features, adj, labels, idx_test)\n",
    "        print(t,'\\t',\"test\",'\\t',loss_test,'\\t',acc_test)\n",
    "        a = open(dataset_name+'_IID_'+str(iid_percent)+'_' + str(L_hop) +'hop_Block_federated_' + str(num_layers) + 'layer_GCN_iter_'+str(args_iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K),'a+')\n",
    "        a.write(str(t)+'\\t'+\"test\"+'\\t'+str(loss_test)+'\\t'+str(acc_test)+'\\n')\n",
    "        a.close()\n",
    "        print(\"save file as\",dataset_name+'_IID_'+str(iid_percent)+'_' + str(L_hop) +'hop_Block_federated_' + str(num_layers) + 'layer_GCN_iter_'+str(args_iterations)+'_epoch_'+str(args_epochs)+'_device_num_'+str(K))\n",
    "        \n",
    "        del global_model\n",
    "        del features \n",
    "        del adj\n",
    "        del labels\n",
    "        del idx_train\n",
    "        del idx_val\n",
    "        del idx_test\n",
    "        while(len(models)>=1):\n",
    "            del models[0]\n",
    "        \n",
    "        return loss_test, acc_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubM8c3SqXwA3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "#'cora', 'citeseer', 'pubmed' #simulate #other dataset twitter, \n",
    "dataset_name=\"cora\"#'ogbn-arxiv'\n",
    "\n",
    "if dataset_name == 'simulate':\n",
    "    number_of_nodes=200\n",
    "    class_num=3\n",
    "    link_inclass_prob=10/number_of_nodes  #when calculation , remove the link in itself\n",
    "    #EGCN good when network is dense 20/number_of_nodes  #fails when network is sparse. 20/number_of_nodes/5\n",
    "\n",
    "    link_outclass_prob=link_inclass_prob/20\n",
    "\n",
    "\n",
    "    features, adj, labels, idx_train, idx_val, idx_test = generate_data(number_of_nodes,  class_num, link_inclass_prob, link_outclass_prob)               \n",
    "else:\n",
    "    features, adj, labels, idx_train, idx_val, idx_test = load_data(dataset_name)\n",
    "    class_num = labels.max().item() + 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name in ['simulate', 'cora', 'citeseer', 'pubmed']:\n",
    "    args_hidden = 16\n",
    "else:\n",
    "    args_hidden = 256\n",
    "\n",
    "args_dropout = 0.5\n",
    "args_lr = 1.0\n",
    "args_weight_decay = 5e-4     #L2 penalty\n",
    "args_epochs = 3\n",
    "args_no_cuda = False\n",
    "args_cuda = not args_no_cuda and torch.cuda.is_available()\n",
    "\n",
    "args_device_num = class_num #split data into args_device_num parts\n",
    "args_iterations = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for testing\n",
    "centralized_GCN(features, adj, labels, idx_train, idx_val, idx_test, num_layers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    centralized_GCN(features, adj, labels, idx_train, idx_val, idx_test, num_layers = 2)\n",
    "    \n",
    "for args_epochs in [3]:\n",
    "    for args_random_assign in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "        for i in range(10):\n",
    "            Lhop_Block_federated_GCN(class_num, features, adj, labels, idx_train, idx_val, idx_test, args_random_assign, 0, num_layers = 2)\n",
    "            BDS_federated_GCN(class_num, features, adj, labels, idx_train, idx_val, idx_test, args_random_assign)\n",
    "            Lhop_Block_federated_GCN(class_num, features, adj, labels, idx_train, idx_val, idx_test, args_random_assign, 1, num_layers = 2)\n",
    "            Lhop_Block_federated_GCN(class_num, features, adj, labels, idx_train, idx_val, idx_test, args_random_assign, 2, num_layers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_l in range(1, 11):\n",
    "    for i in range(10):#10 times\n",
    "        centralized_GCN(features, adj, labels, idx_train, idx_val, idx_test, num_layers = num_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for args_epochs in [3]:\n",
    "    for args_random_assign in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "        for num_l in range(2, 11):\n",
    "            for i in range(10):\n",
    "                Block_federated_multilayer_GCN(class_num, features, adj, labels, idx_train, idx_val, idx_test, args_random_assign, num_layers = num_l)\n",
    "                Lhop_Block_federated_multilayer_GCN(class_num, features, adj, labels, idx_train, idx_val, idx_test, args_random_assign, 1, num_layers = num_l)\n",
    "                Lhop_Block_federated_multilayer_GCN(class_num, features, adj, labels, idx_train, idx_val, idx_test, args_random_assign, 2, num_layers = num_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "main.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
